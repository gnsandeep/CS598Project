{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eight-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "import sklearn.metrics as sklm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consolidated-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baking-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "textile-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(224),\n",
    "            # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "            # 224\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean, std)\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean, stda)\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "powered-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path_to_images,labelcsv,transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.path_to_images = path_to_images\n",
    "        #self.df = pd.read_csv(\"nih_labels.csv\")\n",
    "        self.df = pd.read_csv(labelcsv)\n",
    "        #self.df = self.df[self.df['fold'] == fold]\n",
    "\n",
    "        self.df = self.df.set_index(\"Image Index\")\n",
    "        self.PRED_LABEL = [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly',\n",
    "            'Effusion',\n",
    "            'Infiltration',\n",
    "            'Mass',\n",
    "            'Nodule',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Emphysema',\n",
    "            'Fibrosis',\n",
    "            'Pleural_Thickening',\n",
    "            'Hernia']\n",
    "\n",
    "        RESULT_PATH = \"results/\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.path_to_images,\n",
    "                self.df.index[idx]))\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        label = np.zeros(len(self.PRED_LABEL), dtype=int)\n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "             # can leave zero if zero, else make one\n",
    "            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') > 0):\n",
    "                label[i] = self.df[self.PRED_LABEL[i].strip()\n",
    "                                   ].iloc[idx].astype('int')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label,self.df.index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sweet-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=None):\n",
    "\n",
    "    data_train = CustomDataset(\n",
    "        path_to_images='/home/ubuntu/payload/NIHData/images/',\n",
    "        labelcsv = 'train_0.1.csv',\n",
    "        transform=data_transforms['train'])\n",
    "    data_val = CustomDataset(\n",
    "        path_to_images='/home/ubuntu/payload/NIHData/images/',\n",
    "        labelcsv = 'val-small_0.1.csv',\n",
    "        transform=data_transforms['val'])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data_val, batch_size=32, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader , data_train , data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "promising-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281, 71)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader , train_dataset , val_dataset = load_data()\n",
    "\n",
    "assert type(train_loader) is torch.utils.data.dataloader.DataLoader\n",
    "len(train_loader),len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriented-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,20,5,1)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.conv3 = nn.Conv2d(50,50,4,1)\n",
    "        self.fc1 = nn.Linear(25*25*50,500)\n",
    "        self.droupout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(500,14)\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1,25*25*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droupout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "played-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "#model = get_cnn_model()\n",
    "model.cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handed-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "def train_model(model, train_dataloader,val_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    :param model: A CNN model\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "    TODO:\n",
    "        Within the loop, do the normal training procedures:\n",
    "            pass the input through the model\n",
    "            pass the output through loss_func to compute the loss (name the variable as *loss*)\n",
    "            zero out currently accumulated gradient, use loss.basckward to backprop the gradients, then call optimizer.step\n",
    "    \"\"\"\n",
    "    # prep model for training\n",
    "\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        curr_epoch_loss = []\n",
    "        #for data, target in train_dataloader:\n",
    "        for data in train_dataloader:\n",
    "            # your code here\n",
    "            #inputs, labels, _ = data\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"label is cuda :\", labels.is_cuda) \n",
    "            #y_hat = model(data)\n",
    "            y_hat = model(inputs)\n",
    "            #print(\"y_hat is cuda :\", y_hat.is_cuda)\n",
    "            y_hat = y_hat.to(device)\n",
    "            #print(\"y_hat is cuda :\", y_hat.is_cuda)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            #print(\"label is cuda :\", labels.is_cuda)\n",
    "            #print(y_hat)\n",
    "            #print(labels)\n",
    "            #print(type(labels))\n",
    "            #print((y_hat.shape))\n",
    "            #print((target.shape))\n",
    "            #loss = criterion(y_hat, target)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #curr_epoch_loss.append(loss.item())\n",
    "            #raise NotImplementedError\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: Train curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "        model.eval()\n",
    "        val_curr_epoch_loss = []\n",
    "        for data in val_dataloader:    \n",
    "            #inputs, labels, _ = data\n",
    "            \n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            \n",
    "            #labels = labels.type(torch.FloatTensor)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            #true_labels = labels.cpu().data.numpy()\n",
    "            \n",
    "            #batch_size = true_labels.shape\n",
    "            y_hat = model(inputs)\n",
    "            y_hat = y_hat.to(device)\n",
    "            #probs = y_hat.cpu().data.numpy()\n",
    "                \n",
    "            \n",
    "            loss = criterion(y_hat, labels)\n",
    "            val_curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: Val curr_epoch_loss={np.mean(val_curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heard-colon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train curr_epoch_loss=0.6655983924865723\n",
      "Epoch 0: Val curr_epoch_loss=0.6185768246650696\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader , train_dataset , val_dataset = load_data()\n",
    "model = train_model(model, train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    :param model: A CNN model\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "    TODO:\n",
    "        Within the loop, do the normal training procedures:\n",
    "            pass the input through the model\n",
    "            pass the output through loss_func to compute the loss (name the variable as *loss*)\n",
    "            zero out currently accumulated gradient, use loss.basckward to backprop the gradients, then call optimizer.step\n",
    "    \"\"\"\n",
    "    model.train() # prep model for training\n",
    "\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        #for data, target in train_dataloader:\n",
    "        for data in train_dataloader:\n",
    "            # your code here\n",
    "            #inputs, labels, _ = data\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #print(\"label is cuda :\", labels.is_cuda) \n",
    "            #y_hat = model(data)\n",
    "            y_hat = model(inputs)\n",
    "            #print(\"y_hat is cuda :\", y_hat.is_cuda)\n",
    "            y_hat = y_hat.to(device)\n",
    "            #print(\"y_hat is cuda :\", y_hat.is_cuda)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            #print(\"label is cuda :\", labels.is_cuda)\n",
    "            #print(y_hat)\n",
    "            #print(labels)\n",
    "            #print(type(labels))\n",
    "            #print((y_hat.shape))\n",
    "            #print((target.shape))\n",
    "            #loss = criterion(y_hat, target)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #curr_epoch_loss.append(loss.item())\n",
    "            #raise NotImplementedError\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader , train_dataset , val_dataset = load_data()\n",
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "classified-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        Y_pred: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert Y_pred and Y_test to numpy arrays (of shape (n_data_points, 2))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #Y_pred = []\n",
    "    #Y_test = []\n",
    "    pred_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "    true_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "    #for data, target in dataloader:\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # your code here\n",
    "        inputs = data[0].to(device)\n",
    "        labels = data[1].to(device)\n",
    "        #inputs, labels, _ = data\n",
    "        true_labels = labels.cpu().data.numpy()\n",
    "        batch_size = true_labels.shape\n",
    "        #print(\"batch_size : \" , batch_size)\n",
    "        y_hat = model(inputs)\n",
    "        probs = y_hat.cpu().data.numpy()\n",
    "        #y_hat = model(data)\n",
    "        #y_hat_ = torch.max(y_hat,dim=1)\n",
    "        #_, predicted = torch.max(y_hat, 1)\n",
    "        #print(type(y_hat),y_hat)\n",
    "        #print(\"predicted : \" ,type(predicted),predicted.shape)\n",
    "        #print(\"target : \" ,target)\n",
    "        #Y_pred.append(predicted.detach().numpy())\n",
    "        #Y_test.append(target.detach().numpy())\n",
    "        for j in range(0, batch_size[0]):\n",
    "            thisrow = {}\n",
    "            truerow = {}\n",
    "            thisrow[\"Image Index\"] = val_dataset.df.index[BATCH_SIZE * i + j]\n",
    "            truerow[\"Image Index\"] = val_dataset.df.index[BATCH_SIZE * i + j]\n",
    "\n",
    "            # iterate over each entry in prediction vector; each corresponds to\n",
    "            # individual label\n",
    "            for k in range(len(val_dataset.PRED_LABEL)):\n",
    "                thisrow[\"prob_\" + val_dataset.PRED_LABEL[k]] = probs[j, k]\n",
    "                truerow[val_dataset.PRED_LABEL[k]] = true_labels[j, k]\n",
    "\n",
    "            pred_df = pred_df.append(thisrow, ignore_index=True)\n",
    "            true_df = true_df.append(truerow, ignore_index=True)\n",
    "\n",
    "        if(i % 10 == 0):\n",
    "            print(str(i * BATCH_SIZE))\n",
    "\n",
    "    #print()\n",
    "        #raise NotImplementedError\n",
    "    #Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    #Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_df, true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indirect-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bacterial-cancellation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "320\n",
      "640\n",
      "960\n",
      "1280\n",
      "1600\n",
      "1920\n",
      "2240\n"
     ]
    }
   ],
   "source": [
    "pred_df, true_df = eval_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "italian-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df = pd.DataFrame(columns=[\"label\", \"auc\"])\n",
    "for column in true_df:\n",
    "    #print('---------------------')\n",
    "    #print(\"Column : \" , column)\n",
    "    if column not in [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly',\n",
    "            'Effusion',\n",
    "            'Infiltration',\n",
    "            'Mass',\n",
    "            'Nodule',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Emphysema',\n",
    "            'Fibrosis',\n",
    "            'Pleural_Thickening',\n",
    "                'Hernia']:\n",
    "        continue\n",
    "    actual = true_df[column]\n",
    "    pred = pred_df[\"prob_\" + column]\n",
    "    #print('Pred : ' , pred)\n",
    "    thisrow = {}\n",
    "    thisrow['label'] = column\n",
    "    thisrow['auc'] = np.nan\n",
    "    try:\n",
    "        thisrow['auc'] = sklm.roc_auc_score(\n",
    "        actual.values.astype(int), pred.values)\n",
    "        #auc_df = auc_df.append(thisrow, ignore_index=True)\n",
    "    #except Exception:\n",
    "\n",
    "    except BaseException as e:\n",
    "        print(\"can't calculate auc for \" + str(column))\n",
    "        print(e)\n",
    "    auc_df = auc_df.append(thisrow, ignore_index=True)\n",
    "    \n",
    "pred_df.to_csv(\"preds_test.csv\", index=False)\n",
    "auc_df.to_csv(\"aucs_test.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-cornell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvproject_venv",
   "language": "python",
   "name": "dvproject_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
