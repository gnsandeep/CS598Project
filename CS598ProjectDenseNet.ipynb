{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "import sklearn.metrics as sklm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda search torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize(224),\n",
    "            # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "            # 224\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path_to_images,labelcsv,transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.path_to_images = path_to_images\n",
    "        #self.df = pd.read_csv(\"nih_labels.csv\")\n",
    "        self.df = pd.read_csv(labelcsv)\n",
    "        #self.df = self.df[self.df['fold'] == fold]\n",
    "\n",
    "        self.df = self.df.set_index(\"Image Index\")\n",
    "        self.PRED_LABEL = [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly',\n",
    "            'Effusion',\n",
    "            'Infiltration',\n",
    "            'Mass',\n",
    "            'Nodule',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Emphysema',\n",
    "            'Fibrosis',\n",
    "            'Pleural_Thickening',\n",
    "            'Hernia']\n",
    "        \n",
    "        RESULT_PATH = \"results/\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.path_to_images,\n",
    "                self.df.index[idx]))\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        label = np.zeros(len(self.PRED_LABEL), dtype=int)\n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "             # can leave zero if zero, else make one\n",
    "            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') > 0):\n",
    "                label[i] = self.df[self.PRED_LABEL[i].strip()\n",
    "                                   ].iloc[idx].astype('int')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, label,self.df.index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=None):\n",
    "    \n",
    "    data_train = CustomDataset(\n",
    "        path_to_images='/Users/gnsandeep/Documents/CS598/CS598Project/data/temp/train/train/',\n",
    "        labelcsv = 'train_small_updated.csv',\n",
    "        transform=data_transforms['train'])\n",
    "    data_val = CustomDataset(\n",
    "        path_to_images='/Users/gnsandeep/Documents/CS598/CS598Project/data/temp/val/val/',\n",
    "        labelcsv = 'val_small_updated.csv', \n",
    "        transform=data_transforms['val'])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(data_val, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader , data_train , data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader , train_dataset , val_dataset = load_data()\n",
    "\n",
    "assert type(train_loader) is torch.utils.data.dataloader.DataLoader\n",
    "len(train_loader),len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,20,5,1)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.conv3 = nn.Conv2d(50,50,4,1)\n",
    "        self.fc1 = nn.Linear(25*25*50,500)\n",
    "        self.droupout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(500,14)\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        #input is of shape (batch_size=32, 3, 224, 224) if you did the dataloader right\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1,25*25*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droupout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Define the CNN model here. \n",
    "        We will use the pretrained ResNet18 model, which can be initialized with torchvision.models.resnet18\n",
    "        Then, replace the last layer (model.fc) with a nn.Linear layer\n",
    "            The new model.fc should have the same input size but a new output_size of 2\n",
    "    \"\"\"\n",
    "    \n",
    "    from torchvision import models\n",
    "    \n",
    "    num_classes = 14\n",
    "    # your code here\n",
    "    #raise NotImplementedError\n",
    "    #old code model = torchvision.models.resnet18()\n",
    "    model = models.densenet121(pretrained=True)\n",
    "    #old code num_ftrs = model.fc.in_features\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, num_classes), nn.Sigmoid())\n",
    "    # old code model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    #For computation efficiency, we will freeze the weights in the bottom layers\n",
    "    #If it's too slow, you can turn off layer4's weights update as well\n",
    "    '''\n",
    "    for param in model.named_parameters():\n",
    "        if param[0].split(\".\")[0] in {'fc', 'layer4'}: continue\n",
    "        param[1].requires_grad = False'''\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = get_cnn_model()\n",
    "model = SimpleCNN()\n",
    "#model.load_state_dict(torch.load('resnet18_weights_9.pth', map_location='cpu'))\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    \"\"\"\n",
    "    :param model: A CNN model\n",
    "    :param train_dataloader: the DataLoader of the training data\n",
    "    :param n_epoch: number of epochs to train\n",
    "    :return:\n",
    "        model: trained model\n",
    "    TODO:\n",
    "        Within the loop, do the normal training procedures:\n",
    "            pass the input through the model\n",
    "            pass the output through loss_func to compute the loss (name the variable as *loss*)\n",
    "            zero out currently accumulated gradient, use loss.basckward to backprop the gradients, then call optimizer.step\n",
    "    \"\"\"\n",
    "    model.train() # prep model for training\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        #for data, target in train_dataloader:\n",
    "        for data in train_dataloader:\n",
    "            # your code here\n",
    "            inputs, labels, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            #y_hat = model(data)\n",
    "            y_hat = model(inputs)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "            #print(y_hat)\n",
    "            #print(labels)\n",
    "            #print(type(labels))\n",
    "            \n",
    "\n",
    "\n",
    "            #print((y_hat.shape))\n",
    "            #print((target.shape))\n",
    "            #loss = criterion(y_hat, target)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #curr_epoch_loss.append(loss.item())\n",
    "            #raise NotImplementedError\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=0.6850053668022156\n",
      "Epoch 1: curr_epoch_loss=0.6807374358177185\n",
      "Epoch 2: curr_epoch_loss=0.676136314868927\n",
      "Epoch 3: curr_epoch_loss=0.6704983115196228\n",
      "Epoch 4: curr_epoch_loss=0.6638619303703308\n",
      "Epoch 5: curr_epoch_loss=0.6556918025016785\n",
      "Epoch 6: curr_epoch_loss=0.6447728872299194\n",
      "Epoch 7: curr_epoch_loss=0.6303286552429199\n",
      "Epoch 8: curr_epoch_loss=0.6097058057785034\n",
      "Epoch 9: curr_epoch_loss=0.5802041888237\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = load_data()\n",
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        Y_pred: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert Y_pred and Y_test to numpy arrays (of shape (n_data_points, 2))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    #Y_pred = []\n",
    "    #Y_test = []\n",
    "    pred_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "    true_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "    #for data, target in dataloader:\n",
    "    for i, data in enumerate(dataloader):    \n",
    "        # your code here\n",
    "        inputs, labels, _ = data\n",
    "        true_labels = labels.detach().numpy()\n",
    "        batch_size = true_labels.shape\n",
    "        print(\"batch_size : \" , batch_size)\n",
    "        y_hat = model(inputs)\n",
    "        probs = y_hat.detach().numpy()\n",
    "        #y_hat = model(data)\n",
    "        #y_hat_ = torch.max(y_hat,dim=1)\n",
    "        #_, predicted = torch.max(y_hat, 1)\n",
    "        #print(type(y_hat),y_hat)\n",
    "        #print(\"predicted : \" ,type(predicted),predicted.shape)\n",
    "        #print(\"target : \" ,target)\n",
    "        #Y_pred.append(predicted.detach().numpy())\n",
    "        #Y_test.append(target.detach().numpy())\n",
    "        for j in range(0, batch_size[0]):\n",
    "            thisrow = {}\n",
    "            truerow = {}\n",
    "            thisrow[\"Image Index\"] = val_dataset.df.index[BATCH_SIZE * i + j]\n",
    "            truerow[\"Image Index\"] = val_dataset.df.index[BATCH_SIZE * i + j]\n",
    "\n",
    "            # iterate over each entry in prediction vector; each corresponds to\n",
    "            # individual label\n",
    "            for k in range(len(val_dataset.PRED_LABEL)):\n",
    "                thisrow[\"prob_\" + val_dataset.PRED_LABEL[k]] = probs[j, k]\n",
    "                truerow[val_dataset.PRED_LABEL[k]] = true_labels[j, k]\n",
    "\n",
    "            pred_df = pred_df.append(thisrow, ignore_index=True)\n",
    "            true_df = true_df.append(truerow, ignore_index=True)\n",
    "\n",
    "        if(i % 10 == 0):\n",
    "            print(str(i * BATCH_SIZE))\n",
    "    \n",
    "    #print()\n",
    "        #raise NotImplementedError\n",
    "    #Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    #Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_df, true_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size :  (32, 14)\n",
      "0\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (32, 14)\n",
      "batch_size :  (1, 14)\n"
     ]
    }
   ],
   "source": [
    "pred_df, true_df = eval_model(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Image Index', 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
       "       'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass',\n",
       "       'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't calculate auc for Hernia\n",
      "Only one class present in y_true. ROC AUC score is not defined in that case.\n"
     ]
    }
   ],
   "source": [
    "auc_df = pd.DataFrame(columns=[\"label\", \"auc\"])\n",
    "\n",
    "for column in true_df:\n",
    "    #print('---------------------')\n",
    "    #print(\"Column : \" , column)\n",
    "    if column not in [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly',\n",
    "            'Effusion',\n",
    "            'Infiltration',\n",
    "            'Mass',\n",
    "            'Nodule',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Emphysema',\n",
    "            'Fibrosis',\n",
    "            'Pleural_Thickening',\n",
    "                'Hernia']:\n",
    "        continue\n",
    "    actual = true_df[column]\n",
    "    #print( 'Actual : ' , actual)\n",
    "    pred = pred_df[\"prob_\" + column]\n",
    "    #print('Pred : ' , pred)\n",
    "    thisrow = {}\n",
    "    thisrow['label'] = column\n",
    "    thisrow['auc'] = np.nan\n",
    "    try:\n",
    "        thisrow['auc'] = sklm.roc_auc_score(\n",
    "        actual.values.astype(int), pred.values)\n",
    "        #auc_df = auc_df.append(thisrow, ignore_index=True)\n",
    "    #except Exception:\n",
    "        \n",
    "    except BaseException as e:\n",
    "        print(\"can't calculate auc for \" + str(column))\n",
    "        print(e)\n",
    "    auc_df = auc_df.append(thisrow, ignore_index=True)\n",
    "\n",
    "pred_df.to_csv(\"preds.csv\", index=False)\n",
    "auc_df.to_csv(\"aucs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
